<!DOCTYPE html>
<html>
<head>
  <title>Bayesian Data Analysis for Medical Data</title>

  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="generator" content="pandoc" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">

  <link rel="stylesheet" media="all" href="main_files/ioslides-13.5.1/fonts/fonts.css">

  <link rel="stylesheet" media="all" href="main_files/ioslides-13.5.1/theme/css/default.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="main_files/ioslides-13.5.1/theme/css/phone.css">

  <base target="_blank">

  <script type="text/javascript">
    var SLIDE_CONFIG = {
      // Slide settings
      settings: {
                title: 'Bayesian Data Analysis for Medical Data',
                        subtitle: 'Bayesian Inference',
                useBuilds: true,
        usePrettify: true,
        enableSlideAreas: true,
        enableTouch: true,
                favIcon: 'main_files/logo.jpg',
              },

      // Author information
      presenters: [
            {
        name:  'Paola Berchialla' ,
        company: '',
        gplus: '',
        twitter: '',
        www: '',
        github: ''
      },
            ]
    };
  </script>

  <style type="text/css">

    b, strong {
      font-weight: bold;
    }

    em {
      font-style: italic;
    }

    slides > slide {
      -webkit-transition: all 0.4s ease-in-out;
      -moz-transition: all 0.4s ease-in-out;
      -o-transition: all 0.4s ease-in-out;
      transition: all 0.4s ease-in-out;
    }

    .auto-fadein {
      -webkit-transition: opacity 0.6s ease-in;
      -webkit-transition-delay: 0.4s;
      -moz-transition: opacity 0.6s ease-in 0.4s;
      -o-transition: opacity 0.6s ease-in 0.4s;
      transition: opacity 0.6s ease-in 0.4s;
      opacity: 0;
    }

    slides > slide:not(.nobackground):before {
      font-size: 12pt;
      content: "";
      position: absolute;
      bottom: 20px;
      left: 60px;
      background: url(main_files/logo.jpg) no-repeat 0 50%;
      -webkit-background-size: 30px 30px;
      -moz-background-size: 30px 30px;
      -o-background-size: 30px 30px;
      background-size: 30px 30px;
      padding-left: 40px;
      height: 30px;
      line-height: 1.9;
    }
  </style>

  <link rel="stylesheet" href="assets\css\ioslides.css" type="text/css" />


</head>

<body style="opacity: 0">

<slides class="layout-widescreen">

  <slide class="title-slide segue nobackground">
        <aside class="gdbar"><img src="main_files/logo.jpg"></aside>
        <!-- The content of this hgroup is replaced programmatically through the slide_config.json. -->
    <hgroup class="auto-fadein">
      <h1 data-config-title><!-- populated from slide_config.json --></h1>
      <h2 data-config-subtitle><!-- populated from slide_config.json --></h2>
      <p data-config-presenter><!-- populated from slide_config.json --></p>
          </hgroup>
  </slide>

<slide class=''><hgroup><h2>Quiz!</h2></hgroup><article  id="quiz">

<p><img src="main_files/figure-html/quiz-1.png" title="" alt="" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>Quiz!</h2></hgroup><article  id="quiz-1">

<p><img src="main_files/figure-html/quiz2-1.png" title="" alt="" width="720" style="display: block; margin: auto;" /></p>

</article></slide><slide class=''><hgroup><h2>Quiz Answer</h2></hgroup><article  id="quiz-answer">

<ul>
<li>Mix simulation of repeated i.i.d trials with 20% of success and sort</li>
</ul>

<pre >## 0/10 1/10 1/10 14/100 15/100 
##  
##  18/100 19/100 192/1000 193/1000 195/1000 
##  
##  196/1000 198/1000 2/10 20/100 20/100 
##  
##  20/100 205/1000 206/1000 207/1000 217/1000 
##  
##  223/1000 23/100 26/100 28/100 3/10 
##  
##  3/10 3/10 4/10 4/10 4/10</pre>

<ul class = 'build'>
<li>More variation in observed rates with smaller sample sizes</li>
</ul>

<ul class = 'build'>
<li><em>Answer:</em> High cancer and low cancer counties are small populations</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Bayesian Mechanics</h2></hgroup><article  id="bayesian-mechanics">

<!-- THIS IS A COMMENTED TEXT EVEN FOR MARKDOWN :-) -->

<ol>
<li>Set up full probability model

<ul>
<li>for all observable &amp; unobservable quantities</li>
<li>consistent with problem knowledge &amp; data collection</li>
</ul></li>
<li>Condition on observed data

<ul>
<li>calculate posterior probability of unobserved quantities conditional on observed quantities</li>
</ul></li>
<li>Evaluate

<ul>
<li>model fit</li>
<li>implications of posterior</li>
</ul></li>
</ol>

</article></slide><slide class=''><hgroup><h2>Notation</h2></hgroup><article  id="notation">

<ul>
<li>Basic Quantities

<ul>
<li>\(y\): observed data</li>
<li>\(\theta\): parameters (and other unobserved quantities)</li>
<li>\(x\): constants, predictors for conditional models</li>
</ul></li>
<li>Basic Predictive Quantities

<ul>
<li>\(\tilde y\): unknown, potentially observable quantities</li>
<li>\(\tilde x\): predictors for unknown quantities</li>
</ul></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Naming Conventions</h2></hgroup><article  id="naming-conventions">

<ul>
<li><p><strong>Joint:</strong> \(p(y,\theta)\)</p></li>
<li><strong>Sampling/Likelihood:</strong> \(p(y\vert \theta)\)

<ul>
<li>Sampling is function of \(y\) with \(\theta\) fixed (probability function)</li>
<li>Likelihood is function of \(\theta\) with \(y\) fixed (<em>not</em> probability function)</li>
</ul></li>
<li><strong>Prior:</strong> \(p(\theta)\)</li>
<li><strong>Posterior:</strong> \(p(\theta\vert y)\)</li>
<li><strong>Data Marginal (Evidence):</strong> \(p(y)\)</li>
<li><p><strong>Posterior Predictive:</strong> \(p(\tilde y\vert y)\)</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Bayes&#39;s Rule for Posterior</h2></hgroup><article  id="bayess-rule-for-posterior">



<ul>
<li><em>Inversion:</em> Final result depends only on sampling distribution (likelihood) \(p(y\vert\theta)\) and prior \(p(\theta)\)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Bayes&#39;s Rule up to Proportion</h2></hgroup><article  id="bayess-rule-up-to-proportion">

<ul>
<li><p>If data \(y\) are fixed, then \[
\begin{array}{rcl}
p(\theta\vert y ) &amp; = &amp; \frac{p(y\vert\theta)p(\theta)}{p(y)} \\
\\
&amp; \propto &amp; p(y\vert\theta)p(\theta) \\
\\
&amp; = &amp; p(y,\theta)
\end{array}
\]</p></li>
<li>Posterior proportional to likelihood times prior</li>
<li><p>Equivalently, posterior proportional to joint</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Posterior Predictive Distribution</h2></hgroup><article  id="posterior-predictive-distribution">

<ul>
<li>Predict new data \(\tilde y\) based on observed data \(y\)</li>
<li>Marginalize out parameter from posterior \[
p(\tilde y \vert y) = \int_\Theta p(\tilde y \vert \theta) p(\theta \vert y)d\theta
\] averaging predcitions \(p(\tilde y\vert\theta)\) weighted by posterior \(p(\theta\vert y)\)</li>
<li>\(\Theta = \left\{\theta \vert p(\theta \vert y) &gt;0 \right\}\) is the support of \(p(\theta\vert y)\)</li>
<li><p>For discrete parameters \(\theta\), \[
p(\tilde y \vert y)=\sum_{\theta \in \Theta} p(\tilde y \vert \theta)p(\theta\vert y)
\]</p></li>
<li><p>Can mix continuous and discrete (integral as shorthand)</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Event Probabilities</h2></hgroup><article  id="event-probabilities">

<ul>
<li>Recall that an event A is a collection of outcomes</li>
<li>Suppose event \(A\) is determined by indicator on parameters \[
f(\theta) = \begin{cases} 1 &amp; \mbox{if } \theta \in A \\ 
0 &amp; \mbox{if } \theta \notin A  \end{cases}
\]</li>
<li>e.g. \(f(\theta) = \theta_1 &gt; \theta_2\) for \(Pr(\theta_1 &gt; \theta_2\vert y)\)</li>
<li>Bayesian event probabilities compute posterior mass \[
P(A) = \int_\Theta f(\theta)p(\theta\vert y)d\theta
\]</li>
<li>Not frequentist, because involves parameter probabilities</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Laplace&#39;s Data and Problems</h2></hgroup><article  id="laplaces-data-and-problems">

<ul>
<li>Laplace&#39;s data on live birth in Paris from 1745-1770</li>
</ul>

<table class = 'rmdtable'>
<tr class="header">
<th align="left"><em>sex</em></th>
<th align="left"><em>live births</em></th>
</tr>
<tr class="odd">
<td align="left">female</td>
<td align="left">241,945</td>
</tr>
<tr class="even">
<td align="left">male</td>
<td align="left">251,527</td>
</tr>
</table>

<ul>
<li><p>Question 1 (Event Probability) Is a boy more likely to be born than a girl?</p></li>
<li><p>Question 2 (Estimate) What is the birth rate of boys vs. girls?</p></li>
<li><p>Bayes formulated the basic binomial model</p></li>
</ul>

<!-- Laplace solved the integral (Bayes couldn't) -->

</article></slide><slide class=''><hgroup><h2>Binomial Distribution</h2></hgroup><article  id="binomial-distribution">

<ul>
<li>Binomial distribution is number of successes \(y\) in \(N\) i.i.d. Bernoulli trials with chance of success \(\theta\)</li>
<li>If \(y_1,\ldots , y_N\sim\) Bernoulli(\(\theta\)), then \((y_1 + \ldots + y_N)\sim\) Binomial(\(N,\theta\))</li>
<li>The probabilistic model is \[
Binomial (y\vert N, \theta) = {{N}\choose{y}}\theta^y(1-\theta)^{N-y}
\] where the binomial coefficient normalizes for permutation (i.e., which subset of \(n\) has \(y_n =1\)): \[
{{N}\choose{y}} = \frac{N!}{y!(N-y)!}
\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Bayes&#39;s Binomial Model</h2></hgroup><article  id="bayess-binomial-model">

<ul>
<li>Data

<ul>
<li>\(y:\) total number of male live births (241,945)</li>
<li>\(N:\) total number of live births (493,472)</li>
</ul></li>
<li>Parameter

<ul>
<li>\(\theta \in (0,1)\): proportion of male live births</li>
</ul></li>
<li>Likelihood \[
p(y\vert N,\theta) = Binomial(y\vert N,\theta) ={{N}\choose{y}}\theta^y(1-\theta)^{N-y}
\]</li>
<li>Prior \[
p(\theta) = Uniform(\theta\vert 0,1)=1
\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Beta Distribution</h2></hgroup><article  id="beta-distribution">

<ul>
<li>Required for analytic posterior of Bayes&#39;s model</li>
<li>For parameters \(\alpha, \beta&gt;0\) and \(\theta\in (0,1)\), \[
Beta(\theta\vert \alpha, \beta) = \frac{1}{B(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}
\]</li>
<li>Euler&#39;s Beta function is used to normalize \[
Beta(\theta\vert \alpha, \beta) = \int_0^1 u^{\alpha-1}(1-u)^{\beta-1}du = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
\] where \(\Gamma()\) is continuous generalization of factorial</li>
<li>Note: \(Beta(\theta\vert 1,1)=Uniform(\theta, 0,1)\)</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Beta Distribution examples</h2></hgroup><article  id="beta-distribution-examples">

<p><img src="main_files/figure-html/unnamed-chunk-2-1.png" title="" alt="" width="960" /></p>

</article></slide><slide class=''><hgroup><h2>Posterior distribution</h2></hgroup><article  id="posterior-distribution">

<ul>
<li>Given Baye&#39;s general formula for the posterior \[
p(\theta\vert y, N)=\frac{Binomial(y\vert N,\theta)Uniform(\theta\vert 0,1)}{\int_\Theta Binomial(y\vert N,\theta&#39;)p(\theta&#39;)d\theta&#39;}
\]</li>
<li>Using Euler&#39;s Beta function to normalize the posterior with final solution \[
p(\theta\vert y, N)= Beta(\theta\vert y+1, N-y+1)
\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Estimation</h2></hgroup><article  id="estimation">

<ul>
<li>Posterior is \(Beta(\theta \vert 1+ 241,945, 1+ 251,527)\)</li>
<li>Posterior mean: \[
\frac{1+ 241,945}{1+ 241,945+1+ 251,527}\approx 0.4902913
\]</li>
<li>Maximum likelihood estimate same as posterior mode (because of uniform prior) \[
\frac{241,945}{241,945+251,527}\approx 0.4902912
\]</li>
<li>As number of observations \(\to \infty\), MLE approaches to posterior mean</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Event Probability Inference</h2></hgroup><article  id="event-probability-inference">

<ul>
<li>What is probability that a male live birth is more likely than a female live birth?</li>
</ul>

<p>\[
\begin{array}{rcl}
Pr[\theta&gt;0.5] &amp; = &amp; \int_\Theta I[\theta&gt;0.5]p(\theta\vert y,N)d\theta \\
\\
&amp; = &amp; \int_{0.5}^1 p(\theta\vert y,N)d\theta \\
\\
&amp; = &amp; 1- F_{\theta\vert y,N}(0.5))\\
\\
&amp;\approx &amp; 1^{-42}
\end{array}
\] - \(I[\theta&gt;0.5] =1\) if \(\theta&gt;0.5\) is true and 0 otherwise - \(F_{\theta\vert y,N}\) is posterior cumulative distribution function (cdf)</p>

</article></slide><slide class=''><hgroup><h2>Bayesian Fisher Exact Test</h2></hgroup><article  id="bayesian-fisher-exact-test">

<ul>
<li>Suppose we observe the following data on handedness</li>
</ul>

<table class = 'rmdtable'>
<tr class="header">
<th align="left"><em>Drug</em></th>
<th align="left"><em>AEs</em></th>
<th align="left"><em>Successes</em></th>
<th align="left">Total</th>
</tr>
<tr class="odd">
<td align="left"><em>A</em></td>
<td align="left">9\((y_1)\)</td>
<td align="left">43</td>
<td align="left">52 \((N_1)\)</td>
</tr>
<tr class="even">
<td align="left"><em>B</em></td>
<td align="left">4\((y_2)\)</td>
<td align="left">44</td>
<td align="left">48\((N_2)\)</td>
</tr>
</table>

<ul>
<li>Assume likelihoods \(Binomial(y_k\vert N_k, \theta_k)\), uniform priors</li>
<li><p>Is Drug B better than A? \[
\begin{array}{rcl}
Pr[\theta_1 &gt; \theta_2 \vert y, N] &amp;=&amp;
\int_0^1 \int_0^1 p(\theta_1\vert y_1, N_1)p(\theta_2\vert y_2, N_2)d\theta_1d\theta_2\\
 &amp;\approx &amp; 0.91
 \end{array}
\]</p></li>
<li><p>Directly interpretable result; <em>not</em> frequentist procedure</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference</h2></hgroup><article  id="visualizing-posterior-difference">

<ul>
<li>Plot of posterior difference \(p(\theta_2 -\theta_1 \vert y N)\) (B-A)</li>
</ul>

<p><img src="main_files/figure-html/unnamed-chunk-3-1.png" title="" alt="" width="720" /></p>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference</h2></hgroup><article  id="visualizing-posterior-difference-1">

<ul>
<li>Plot of posterior difference \(p(\theta_2 -\theta_1 \vert y N)\) (B-A)</li>
</ul>

<p><img src="main_files/figure-html/unnamed-chunk-4-1.png" title="" alt="" width="720" /></p>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference</h2></hgroup><article  id="visualizing-posterior-difference-2" class="smaller">

<pre class = 'prettyprint lang-r'>library(R2OpenBUGS)</pre>

<pre >## Warning: package &#39;R2OpenBUGS&#39; was built under R version 3.2.4</pre>

<pre class = 'prettyprint lang-r'>mymodel = function(){
  y1~ dbin(theta1, N1)
  y2~ dbin(theta2, N2)
  theta1~dbeta(1,1)
  theta2~dbeta(1,1)
  theta &lt;- theta2-theta1
}

data = list(N1 =52, N2 =48, y1=43, y2 = 44)
parameters=c(&quot;theta&quot;)
bugsInits &lt;- function() {
  return(list(theta1=0.5, theta2=0.5)) 
}

resbugs = bugs(data, inits=bugsInits, parameters.to.save=parameters, model.file=mymodel,
n.chains=1, n.iter=10000, n.burnin=2000, n.thin=1,
                debug=TRUE)</pre>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference</h2></hgroup><article  id="visualizing-posterior-difference-3">

<ul>
<li>Plot of posterior difference \(p(\theta_2 -\theta_1 \vert y N)\) (B-A)</li>
<li>compute 95% credible interval</li>
</ul>

<pre class = 'prettyprint lang-r'>theta.post = resbugs$sims.list$theta
plot(density(theta.post), main= &quot;posterior probability&quot;, 
     xlab=expression(theta[2]-theta[1]) )</pre>

<p><img src="main_files/figure-html/unnamed-chunk-5-1.png" title="" alt="" width="288" /></p>

<pre class = 'prettyprint lang-r'>quantile(theta.post, probs=c(.025, .5, .975))</pre>

<pre >##       2.5%        50%      97.5% 
## -0.0428205  0.0842250  0.2222025</pre>

</article></slide><slide class=''><hgroup><h2>Visualizing Posterior Difference</h2></hgroup><article  id="visualizing-posterior-difference-4">

<ul>
<li>Plot of posterior difference \(p(\theta_2 -\theta_1 \vert y N)\) (B-A)</li>
<li>compute 95% credible interval</li>
</ul>

<pre class = 'prettyprint lang-r'>theta.post = resbugs$sims.list$theta
quantile(theta.post, probs=c(.025, .5, .975))</pre>

<pre >##       2.5%        50%      97.5% 
## -0.0428205  0.0842250  0.2222025</pre>

</article></slide><slide class=''><hgroup><h2>Conjugate Priors</h2></hgroup><article  id="conjugate-priors">

<ul>
<li>Family \(F\) is a conjugate prior for family \(G\) if

<ul>
<li>prior in \(F\) and</li>
<li>likelihood in \(G\)</li>
<li>entails posterior in \(F\)</li>
</ul></li>
<li>Before MCMC techniques became practical, Bayesian analysis mostly involved conjugate priors</li>
<li>Still widely used because analytic solutions are more efficient than MCMC</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Beta is Conjugate to Binomial</h2></hgroup><article  id="beta-is-conjugate-to-binomial">



<p>\[
\begin{array}{rcl}
p(\theta\vert y, N, \alpha,\beta) &amp;\approx&amp; p(\theta\vert\alpha,\beta)p(y\vert N,\theta)\\
\\
&amp;=&amp; Beta(\theta\vert\alpha,\beta)Binomial(y\vert N,\theta)\\
\\
&amp;=&amp; \frac{1}{B(\alpha, \beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}{{N}\choose{y}}\theta^y(1-\theta)^{N-y}\\
\\
&amp;\approx&amp;\theta^{y+\alpha-1}(1-\theta)^{N-y+\beta-1}\\
\\
&amp;\approx&amp;Beta(\theta\vert \alpha + y, \beta +(N-y))
\end{array}
\]</p>

</article></slide><slide class=''><hgroup><h2>Chaining Updates</h2></hgroup><article  id="chaining-updates">

<ul>
<li>Start with prior \(Beta(\theta\vert\alpha,\beta)\)</li>
<li>Receive binomial data in \(K\) stages \((y_1, N_1),\ldots , (y_K, N_K)\)</li>
<li>After \((y_1, N_1)\), posterior is \(Beta(\theta\vert \alpha + y_1, \beta +N_1-y_1)\)</li>
<li>Use as prior for \((y_2, N_2)\) with posterior \[
Beta(\theta\vert \alpha + y_1 +y_2, \beta +(N_1-y_1) + (N_2-y_2))
\]</li>
<li>Lather, rinse, repeate until final posterior \[
Beta(\theta\vert \alpha + y_1 +\ldots +y_K, \beta +(N_1 +\ldots N_K) - (y_1+\ldots +y_K))
\]</li>
<li>Same results as if we had updated with combined data \[
Beta(y_1+\ldots +y_K, N_1 +\ldots N_K)
\]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>MAP Estimator</h2></hgroup><article  id="map-estimator">

<ul>
<li>For a Bayesian model \(p(y,\theta) = p(y\vert\theta)p(\theta)\), the max a posteriori (MAP) estimate maximizes the posterior \[
  \begin{array}{rcl}
\theta^* &amp; = &amp; arg max_\theta p(\theta\vert y)\\
\\
&amp; = &amp; arg max_\theta \frac{p(y\vert \theta)p(\theta)}{p(y)}\\
\\
&amp; = &amp; arg max_\theta p(y\vert \theta)p(\theta)\\
\\
&amp; = &amp; arg max_\theta \log p(y\vert \theta) + \log p(\theta)\\
\end{array}
\]</li>
<li><em>not</em> Bayesian because it does not integrate over uncertainty</li>
<li><em>not</em> frequentist because of distributions over parameters</li>
</ul>

</article></slide><slide class=''><hgroup><h2>MAP and the MLE</h2></hgroup><article  id="map-and-the-mle">

<ul>
<li>MAP estimate reduces to the MLE if the prior is uniform, i.e., \[
  p(\theta) = c
  \] because \[
\begin{array}{rcl}
  \theta^* &amp; = &amp; arg max_\theta p(y\vert \theta)p(\theta)\\
  \\
  &amp; = &amp; arg max_\theta p(y\vert \theta)c\\
  \\
  &amp; = &amp; arg max_\theta p(y\vert \theta)\\
  \end{array}
  \]</li>
</ul>

</article></slide><slide class=''><hgroup><h2>Penalized Maximum Likelihood</h2></hgroup><article  id="penalized-maximum-likelihood">

<ul>
<li>The MAP can be made palatable to frequentists via philosophical sleight of hand</li>
<li>Treat the negative log prior \(-\log p(\theta)\) as a <em>penalty</em></li>
<li>e.g. a \(Normal(\theta\vert\mu,\sigma)\) prior becomes a penalty function \[
\lambda_{\theta,\mu,\sigma} = - \left(
  \log\sigma + \frac{1}{2}\left(\frac{\theta-\mu}{\sigma}\right)^2\right)
\]

<ul>
<li>Maximize sum of log likelihood and negative penalty</li>
</ul></li>
</ul>

<p>\[
  \begin{array}{rcl}
  \theta^* &amp; = &amp; arg max_\theta \log p(y\vert \theta, x) - \lambda_{\theta,\mu,\sigma}\\
    \\
  &amp; = &amp; arg max_\theta \log p(y\vert \theta, x) + \log p(\theta\vert\mu,\sigma)
  \end{array}
  \]</p>

</article></slide><slide class=''><hgroup><h2>Proper Bayesian Point Estimates</h2></hgroup><article  id="proper-bayesian-point-estimates">

<ul>
<li><p>Choose estimate to minimize some loss function</p></li>
<li><p>To minimize expected squared error (L2 loss), \(E[(\theta - \theta^{&#39;})^2]\), use the posterior mean</p></li>
</ul>

<p>\[
\hat\theta = arg min_\theta E[(\theta - \theta^{&#39;})^2] = \int_\Theta \theta\times p(\theta\vert y)d\theta
\]</p>

<ul>
<li><p>To minimize expected absolute error (L1 loss), \(E[|\theta - \theta^{&#39;}|]\), use posterior median</p></li>
<li><p>Other loss (utility) functions possible, study of which falls under decision theory</p></li>
<li><p>All share property of involving full Bayesian inference.</p></li>
</ul>

</article></slide><slide class=''><hgroup><h2>Getting the slides</h2></hgroup><article  id="getting-the-slides">

<ul>
<li>The slides for this course were created with Rmarkdown: <a href='http://rmarkdown.rstudio.com/' title=''><a href='http://rmarkdown.rstudio.com/' title=''>http://rmarkdown.rstudio.com/</a></a>.</li>
<li>They are available from <a href='https://github.com/berkeley3/BDA' title=''><a href='https://github.com/berkeley3/BDA' title=''>https://github.com/berkeley3/BDA</a></a>.</li>
<li><p>To re-compile the slides:</p>

<ul>
<li>Download the directory containing the lectures from Github</li>
<li>In R open the .Rmd file and set the working directory to the lecture directory</li>
<li>Click the <em>KnitHTML</em> button on Rstudio or run the following commands:</li>
</ul></li>
</ul>

<pre class = 'prettyprint lang-r'>library(rmarkdown) 
render(&quot;main.Rmd&quot;)</pre></article></slide>


  <slide class="backdrop"></slide>

</slides>

<script src="main_files/ioslides-13.5.1/js/modernizr.custom.45394.js"></script>
<script src="main_files/ioslides-13.5.1/js/prettify/prettify.js"></script>
<script src="main_files/ioslides-13.5.1/js/prettify/lang-r.js"></script>
<script src="main_files/ioslides-13.5.1/js/prettify/lang-yaml.js"></script>
<script src="main_files/ioslides-13.5.1/js/hammer.js"></script>
<script src="main_files/ioslides-13.5.1/js/slide-controller.js"></script>
<script src="main_files/ioslides-13.5.1/js/slide-deck.js"></script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "main_files/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!-- map slide visiblity events into shiny -->
<script>
  (function() {
    if (window.jQuery) {
       window.jQuery(document).on('slideleave', function(e) {
         window.jQuery(e.target).trigger('hidden');
      });
       window.jQuery(document).on('slideenter', function(e) {
         window.jQuery(e.target).trigger('shown');
      });
    }
  })();
</script>

</body>
</html>
