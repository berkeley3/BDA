---
title: "The Predictive Distribution and Model Checking"
subtitle: subtitle
author      : Paola Berchialla
job         : Department of Clinical and Biological Sciences, University of Torino
output:
  ioslides_presentation:
    css: assets/css/ioslides.css
    logo: assets/img/dscb.jpg
    mathjax: local
    self_contained: no
    widescreen: yes
  beamer_presentation: default
  slidy_presentation: default
---

## Predictive distributions

- Predictions of future observables are based on _predictive distributions_
    - the distribution of the data averaged over all possible parameter values

- __Prior predictive distribution__
$$
f(\mathbf y) = \int f(\mathbf y \vert \boldsymbol{\theta}) f(\boldsymbol{\theta}) d\boldsymbol{\theta}
$$

- __(Posterior) predictive distribution__

$$
f(\mathbf y'\vert \mathbf y) = \int f(\mathbf y' \vert \boldsymbol{\theta}) f(\boldsymbol{\theta}\vert\mathbf y) d\boldsymbol{\theta}
$$

## Using predictive distribution for model evaluation

- The predictive distribution $f(\mathbf y'\vert\mathbf, m)$ of a model $m$ is used for checking model assumptions and goodness-of-fit
    - replicated $\mathbf y^{rep}$ can be easily generate
    - add a single step within any MCMC sampler using $f\left( y^{rep}\vert \boldsymbol{\theta}^{(t)}\right)$
    - $\boldsymbol{\theta}^{(t)}$: the parameter values of the current state of the algorithm
    
## Predictive data

- The predictive data $\mathbf y^{rep}$ reflect the expected observations after replicating the study:
    - having already observed $\mathbf y$ 
    - assuming the model adopted is true

- Comparison can be made using summary functions $D(\mathbf y, \boldsymbol{\theta})$
    - test statistics for checking the assumptions and measure discrepancies between data and model (Gelman, 1996)
    
    - _Posterior predictive p-values (PPP)_ (Meng 1994)
$$
P\left(D(\mathbf y^{\text{rep}}, \boldsymbol{\theta}) > D(\mathbf y, \boldsymbol{\theta}) \vert \mathbf y \right)
$$
 
- Values of PPP around 0.5 indicate the distributions of replicated and actual data are close
- Values of PPP close to 0 indicate  differences the distributions of replicated and actual data

## Beware

- In model checks, data are used twice:
    - 1st: estimation of the posterior predictive density
    - 2nd: comparison between the predictive density and the data

- __Violation of the likelihood principle__
    - not entirely a violation if the posterior predictive checks are used only as measure of discrepancy between the model and the data to identify poorly fitted models and not for model comparison and inference

## Model checks

Model checks can be divided into:

- individual checks
    - based on each $y_i$ and $y^{rep}_i$ separately to trace outliers or surprising observations (under the assumed model)
- overall predictive diagnostics to check general assumptions of the model: 
    - normality, goodness-of-fit

## Steps for posterior predictive checks

- Plot and compare the frequency tabulations of $\mathbf y^{\text{rep}}$ and $\mathbf y$ for discrete data

- Plot and compare the cumulative frequencies of $\mathbf y^{\text{rep}}$ and $\mathbf y$ for continuous data

- Plot and compare ordered data $\left(y_{(1)}^{\text{rep}}, \ldots, y_{(n)}^{\text{rep}} \right)$ and $\left(y_{(1)}, \ldots, y_{(n)} \right)$ for continuous data

- Plot estimated posterior predictive ordinate $f(y_i\vert\mathbf y)$ against $y_i$ to trace surprising values

## Steps for posterior predictive checks

- The posterior predictive ordinate
$$
PPO_i = f(y_i\vert\mathbf y) = \int f(y_i\vert\boldsymbol{\theta}) f(\boldsymbol{\theta}\vert\mathbf y) d\boldsymbol{\theta}
$$
provides the probability of observing $y_i$ after having observed $\mathbf y$
    - small values indicate observations originating from the tail
    - extremely small values indicate potential outliers
    - a large amount of $y_i$ with small PPO may indicate a poorly fitted model

## Steps for posterior predictive checks

- Use test statistics and posterior p-values to quantify differences concerning
    - outliers: individual test statistics on the basis of residual values
    - structural assumptions of the model: global test statistics, i.e. comparing the skewness and the kurtosis of $\mathbf y^{\text{rep}}$ with the corresponding observed measures
    - fitness of the model: usual measure such as $\chi^2$

$$
\chi^2(\mathbf y, \boldsymbol{\theta}) = \sum_{i=1}^n \frac{[y_i - E(Y_i\boldsymbol{\theta})]^2}{\text{Var} E(Y_i\boldsymbol{\theta})}
$$
  and deviance
$$
Deviance(\mathbf y, \boldsymbol{\theta}) = -2 \sum_{i=1}^n \text{log} f(y_i\vert\boldsymbol{\theta})
$$


## References

- Gelman A, Meng XL, Stern H (1996). Posterior predictive assessment of model fitness via realized discrepancies. _Statistica Sinica_ __6__, 733-807

- Meng XL (1994). Posterior predictive p-values. _Annals of Statistics_ __22__, 1142-1160

## Getting the slides

* The slides for this course were created with Rmarkdown: [http://rmarkdown.rstudio.com/](http://rmarkdown.rstudio.com/). 
* They are available from [https://github.com/berkeley3/BDA](https://github.com/berkeley3/BDA).
* To re-compile the slides:

    + Download the directory containing the lectures from Github
    + In R open the .Rmd file and set the working directory to the lecture directory
    + Click the *KnitHTML* button on Rstudio or run the following commands: 
  
```{r RmarkdownChunk, eval=FALSE}
library(rmarkdown) 
render("main.Rmd")
